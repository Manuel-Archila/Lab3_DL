<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="lab3_files/libs/clipboard/clipboard.min.js"></script>
<script src="lab3_files/libs/quarto-html/quarto.js"></script>
<script src="lab3_files/libs/quarto-html/popper.min.js"></script>
<script src="lab3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="lab3_files/libs/quarto-html/anchor.min.js"></script>
<link href="lab3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="lab3_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="lab3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="lab3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="lab3_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="laboratorio-3" class="level1">
<h1>Laboratorio 3</h1>
<p>Sean bienvenidos de nuevo al laboratorio 3 de Deep Learning y Sistemas Inteligentes. Así como en los laboratorios pasados, espero que esta ejercitación les sirva para consolidar sus conocimientos en el tema de Redes Neuronales Recurrentes y LSTM.</p>
<p>Este laboratorio consta de dos partes. En la primera trabajaremos una Red Neuronal Recurrente paso-a-paso. En la segunda fase, usaremos PyTorch para crear una nueva Red Neuronal pero con LSTM, con la finalidad de que no solo sepan que existe cierta función sino también entender qué hace en un poco más de detalle.</p>
<p>Para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitará la calificación, no solo asegurándo que ustedes tengan una nota pronto sino también mostrandoles su nota final al terminar el laboratorio.</p>
<p>Espero que esta vez si se muestren los <em>marks</em>. De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensión y colaboración si algo no funciona como debería.</p>
<p>Al igual que en el laboratorio pasado, estaremos usando la librería de Dr John Williamson et al de la University of Glasgow, además de ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualización de sus calificaciones.</p>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:36.411618Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:36.397922Z&quot;}" data-execution_count="30">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Una vez instalada la librería por favor, recuerden volverla a comentar.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install scikit-image</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.922283Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:36.413615Z&quot;}" data-execution_count="31">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#from IPython import display</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#from base64 import b64decode</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Other imports</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unittest.mock <span class="im">import</span> patch</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> uuid <span class="im">import</span> getnode <span class="im">as</span> get_mac</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jhwutils.checkarr <span class="im">import</span> array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.image_audio <span class="im">as</span> ia</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jhwutils.tick <span class="im">as</span> tick</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lautils.gradeutils <span class="im">import</span> new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">###</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>tick.reset_marks()</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.937667Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:37.924277Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a39756cb52fe963f67e015d4d8fe57a4&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-57de155e9f3409c3&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="32">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seeds</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>seed_ <span class="op">=</span> <span class="dv">2023</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.952666Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:37.940679Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;500bf8639033566b1f628a100f1180ca&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-e0ac5721852fe7fd&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="33">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Celda escondida para utlidades necesarias, por favor NO edite esta celda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="información-del-estudiante-en-dos-variables" class="level6">
<h6 class="anchored" data-anchor-id="información-del-estudiante-en-dos-variables">Información del estudiante en dos variables</h6>
<ul>
<li>carne_1 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_1: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
<li>carne_2 : un string con su carne (e.g.&nbsp;“12281”), debe ser de al menos 5 caracteres.</li>
<li>firma_mecanografiada_2: un string con su nombre (e.g.&nbsp;“Albero Suriano”) que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.967716Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:37.954661Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7cd4a99d7434f922d6754ac890fc97e5&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1dec8918a2e1a2cf&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="34">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_1 = </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_1 = </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># carne_2 = </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># firma_mecanografiada_2 = </span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>carne_1 <span class="op">=</span> <span class="st">"161250"</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_1 <span class="op">=</span> <span class="st">"Manuel Archila"</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>carne_2 <span class="op">=</span> <span class="st">"20240"</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>firma_mecanografiada_2 <span class="op">=</span> <span class="st">"Diego Franco"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.983242Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:37.969262Z&quot;}" data-execution_count="35">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>): </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(carne_1)<span class="op">&gt;=</span><span class="dv">5</span> <span class="kw">and</span> <span class="bu">len</span>(carne_2)<span class="op">&gt;=</span><span class="dv">5</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">0</span>):  </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(firma_mecanografiada_1)<span class="op">&gt;</span><span class="dv">0</span> <span class="kw">and</span> <span class="bu">len</span>(firma_mecanografiada_2)<span class="op">&gt;</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"0"}--> 
         ✓ [0 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="parte-1---construyendo-una-red-neuronal-recurrente" class="level2">
<h2 class="anchored" data-anchor-id="parte-1---construyendo-una-red-neuronal-recurrente">Parte 1 - Construyendo una Red Neuronal Recurrente</h2>
<p><strong>Créditos:</strong> La primera parte de este laboratorio está tomado y basado en uno de los laboratorios dados dentro del curso de “Deep Learning” de Jes Frellsen (DeepLearningDTU)</p>
<p>La aplicación de los datos secuenciales pueden ir desde predicción del clima hasta trabajar con lenguaje natural. En este laboratorio daremos un vistazo a como las RNN pueden ser usadas dentro del modelaje del lenguaje, es decir, trataremos de predecir el siguiente token dada una secuencia. En el campo de NLP, un token puede ser un caracter o bien una palabra.</p>
<section id="representanción-de-tokens-o-texto" class="level3">
<h3 class="anchored" data-anchor-id="representanción-de-tokens-o-texto">Representanción de Tokens o Texto</h3>
<p>Como bien hemos hablado varias veces, la computadora no entiende palabras ni mucho menos oraciones completas en la misma forma que nuestros cerebros lo hacen. Por ello, debemos encontrar alguna forma de representar palabras o caracteres en una manera que la computadora sea capaz de interpretarla, es decir, con números. Hay varias formas de representar un grupo de palabras de forma numérica, pero para fines de este laboratorio vamos a centrarnos en una manera común, llamada “one-hot encoding”.</p>
<section id="one-hot-encoding" class="level4">
<h4 class="anchored" data-anchor-id="one-hot-encoding">One Hot Encoding</h4>
<p>Esta técnica debe resultarles familiar de cursos pasados, donde se tomaba una conjunto de categorías y se les asignaba una columna por categoría, entonces se coloca un 1 si el row que estamos evaluando es parte de esa categoría o un 0 en caso contrario. Este mismo acercamiento podemos tomarlo para representar conjuntos de palabras. Por ejemplo</p>
<pre><code>casa = [1, 0, 0, ..., 0]
perro = [0, 1, 0, ..., 0]</code></pre>
<p>Representar un vocabulario grande con one-hot enconding, suele volverse ineficiente debido al tamaño de cada vector disperso. Para solventar esto, una práctica común es truncar el vocabulario para contener las palabras más utilizadas y representar el resto con un símbolo especial, UNK, para definir palabras “desconocidas” o “sin importancia”. A menudo esto se hace que palabras tales como nombres se vean como UNK porque son raros.</p>
</section>
</section>
<section id="generando-el-dataset-a-usar" class="level3">
<h3 class="anchored" data-anchor-id="generando-el-dataset-a-usar">Generando el Dataset a Usar</h3>
<p>Para este laboratorio usaremos un dataset simplificado, del cual debería ser más sencillo el aprender de él. Estaremos generando secuencias de la forma</p>
<pre><code>a b EOS
a a a a b b b b EOS</code></pre>
<p>Noten la aparición del token “EOS”, el cual es un caracter especial que denota el fin de la secuencia. Nuestro task en general será el predecir el siguiente token <span class="math inline">\(t_n\)</span>, donde este podrá ser “a”, “b”, “EOS”, o “UNK” dada una secuencia de forma <span class="math inline">\({t_1 , ... , t_{n-1}}\)</span>.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:37.999248Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:37.984241Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;d0adba37e43168d88355edd44ad433cb&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-62b6e4727b9bb25c&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="36">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reseed the cell</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data(num_seq<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Genera un grupo de secuencias, la cantidad de secuencias es dada por num_seq</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_seq: El número de secuencias a ser generadas</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Una lista de secuencias</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> []</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_seq):</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Genera una secuencia de largo aleatorio</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        num_tokens <span class="op">=</span> np.random.randint(<span class="dv">1</span>,<span class="dv">12</span>) </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Genera la muestra</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> [<span class="st">'a'</span>] <span class="op">*</span> num_tokens <span class="op">+</span> [<span class="st">'b'</span>] <span class="op">*</span> num_tokens <span class="op">+</span> [<span class="st">'EOS'</span>]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agregamos</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        samples.append(sample)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> generate_data()</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Una secuencia del grupo generado"</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sequences[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Una secuencia del grupo generado
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']</code></pre>
</div>
</div>
</section>
<section id="representación-de-tokens-como-índices" class="level3">
<h3 class="anchored" data-anchor-id="representación-de-tokens-como-índices">Representación de tokens como índices</h3>
<p>En este paso haremos la parte del one-hot encoding. Para esto necesitaremos asignar a cada posible palabra de nuestro vocabulario un índice. Para esto crearemos dos diccionarios, uno que permitirá que dada una palabra nos dirá su representación como “indice” en el vocabulario, y el segundo que irá en dirección contraria.</p>
<p>A estos les llamaremos <code>word_to_idx</code> y <code>idx_to_word</code>. La variable <code>vocab_size</code> nos dirá el máximo de tamaño de nuestro vocabulario. Si intentamos acceder a una palabra que no está en nuestro vocabulario, entonces se le reemplazará con el token “UNK” o su índice correspondiente.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:38.014742Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:38.001753Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6f52a0cd85402df075f20a68ae5f4e35&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5276b445f04c739b&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="37">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> seqs_to_dicts(sequences):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Crea word_to_idx y idx_to_word para una lista de secuencias</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">    sequences: lista de secuencias a usar</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Diccionario de palabra a indice</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Diccionario de indice a palabra</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Int numero de secuencias</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Int tamaño del vocabulario</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lambda para aplanar (flatten) una lista de listas</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    flatten <span class="op">=</span> <span class="kw">lambda</span> l: [item <span class="cf">for</span> sublist <span class="kw">in</span> l <span class="cf">for</span> item <span class="kw">in</span> sublist]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aplanamos el dataset</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    all_words <span class="op">=</span> flatten(sequences)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conteo de las ocurrencias de las palabras</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    word_count <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> all_words:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        word_count[word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ordenar por frecuencia</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    word_count <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(word_count.items()), key<span class="op">=</span><span class="kw">lambda</span> x: <span class="op">-</span>x[<span class="dv">1</span>])</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear una lista de todas las palabras únicas</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    unique_words <span class="op">=</span> [w[<span class="dv">0</span>] <span class="cf">for</span> w <span class="kw">in</span> word_count]</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Agregamos UNK a la lista de palabras</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    unique_words.append(<span class="st">"UNK"</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Conteo del número de secuencias y el número de palabras unicas</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    num_sentences, vocab_size <span class="op">=</span> <span class="bu">len</span>(sequences), <span class="bu">len</span>(unique_words)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear diccionarios mencionados</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    word_to_idx <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: vocab_size<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    idx_to_word <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: <span class="st">'UNK'</span>)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Llenado de diccionarios</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, word <span class="kw">in</span> <span class="bu">enumerate</span>(unique_words):</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para agregar</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># word_to_idx[word] = </span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># idx_to_word[idx] = </span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        word_to_idx[word] <span class="op">=</span> idx</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>        idx_to_word[idx] <span class="op">=</span> word</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> word_to_idx, idx_to_word, num_sentences, vocab_size</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>word_to_idx, idx_to_word, num_sequences, vocab_size <span class="op">=</span> seqs_to_dicts(sequences)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tenemos </span><span class="sc">{</span>num_sequences<span class="sc">}</span><span class="ss"> secuencias y </span><span class="sc">{</span><span class="bu">len</span>(word_to_idx)<span class="sc">}</span><span class="ss"> tokens unicos incluyendo UNK"</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"El indice de 'b' es </span><span class="sc">{</span>word_to_idx[<span class="st">'b'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"La palabra con indice 1 es </span><span class="sc">{</span>idx_to_word[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tenemos 100 secuencias y 4 tokens unicos incluyendo UNK
El indice de 'b' es 1
La palabra con indice 1 es b</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:38.030386Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:38.016244Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e23613d7a17abd6db68772917d07f26d&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-c7aed80352919e68&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="38">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">3</span>):        </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(word_to_idx), <span class="st">'0xc51b9ba8'</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">2</span>):        </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(<span class="bu">len</span>(idx_to_word), <span class="st">'0xc51b9ba8'</span>))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_string(idx_to_word[<span class="dv">0</span>], <span class="st">'0xe8b7be43'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"3"}--> 
         ✓ [3 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"2"}--> 
         ✓ [2 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="representación-de-tokens-como-índices-1" class="level3">
<h3 class="anchored" data-anchor-id="representación-de-tokens-como-índices-1">Representación de tokens como índices</h3>
<p>Como bien sabemos, necesitamos crear nuestro dataset de forma que el se divida en inputs y targets para cada secuencia y luego particionar esto en training, validation y test (80%, 10%, 10%). Debido a que estamso haciendo prediccion de la siguiente palabra, nuestro target es el input movido (shifted) una palabra.</p>
<p>Vamos a usar PyTorch solo para crear el dataset (como lo hicimos con las imagenes de perritos y gatitos de los laboratorios pasados). Aunque esta vez no haremos el dataloader. Recuerden que siempre es buena idea usar un DataLoader para obtener los datos de una forma eficienciente, al ser este un generador/iterador. Además, este nos sirve para obtener la información en batches.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.108394Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:38.031389Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;9e41ed4ad2165904a221567eab31e222&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-186baacdbd91cc05&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="39">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils <span class="im">import</span> data</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dataset(data.Dataset):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, targets):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> inputs</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the size of the dataset</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.targets)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Retrieve inputs and targets at the given index</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> <span class="va">self</span>.inputs[index]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.targets[index]</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X, y</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_datasets(sequences, dataset_class, p_train<span class="op">=</span><span class="fl">0.8</span>, p_val<span class="op">=</span><span class="fl">0.1</span>, p_test<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definimos el tamaño de las particiones</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_train)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    num_val <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_val)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(sequences)<span class="op">*</span>p_test)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dividir las secuencias en las particiones</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    sequences_train <span class="op">=</span> sequences[:num_train]</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    sequences_val <span class="op">=</span> sequences[num_train:num_train<span class="op">+</span>num_val]</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    sequences_test <span class="op">=</span> sequences[<span class="op">-</span>num_test:]</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Funcion interna para obtener los targets de una secuencia</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_inputs_targets_from_sequences(sequences):</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Listas vacias</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        inputs, targets <span class="op">=</span> [], []</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agregar informacion a las listas, ambas listas tienen L-1 palabras de una secuencia de largo L</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pero los targetes están movidos a la derecha por uno, para que podamos predecir la siguiente palabra</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sequence <span class="kw">in</span> sequences:</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>            inputs.append(sequence[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>            targets.append(sequence[<span class="dv">1</span>:])</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> inputs, targets</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener inputs y targes para cada subgrupo</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    inputs_train, targets_train <span class="op">=</span> get_inputs_targets_from_sequences(sequences_train)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    inputs_val, targets_val <span class="op">=</span> get_inputs_targets_from_sequences(sequences_val)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    inputs_test, targets_test <span class="op">=</span> get_inputs_targets_from_sequences(sequences_test)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creación de datasets</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    training_set <span class="op">=</span> dataset_class(inputs_train, targets_train)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>    validation_set <span class="op">=</span> dataset_class(inputs_val, targets_val)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    test_set <span class="op">=</span> dataset_class(inputs_test, targets_test)</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> training_set, validation_set, test_set</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>training_set, validation_set, test_set <span class="op">=</span> create_datasets(sequences, Dataset)</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Largo del training set </span><span class="sc">{</span><span class="bu">len</span>(training_set)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Largo del validation set </span><span class="sc">{</span><span class="bu">len</span>(validation_set)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Largo del test set </span><span class="sc">{</span><span class="bu">len</span>(test_set)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Largo del training set 80
Largo del validation set 10
Largo del test set 10</code></pre>
</div>
</div>
</section>
<section id="one-hot-encodings" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encodings">One-Hot Encodings</h3>
<p>Ahora creemos una función simple para obtener la representación one-hot encoding de dado un índice de una palabra. Noten que el tamaño del one-hot encoding es igual a la del vocabulario. Adicionalmente definamos una función para encodear una secuencia.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.123829Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.111300Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;6eea35ea244f238189afef746c0c3067&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-91e0dff1547fcd06&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="40">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode(idx, vocab_size):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Encodea una sola palabra dado su indice y el tamaño del vocabulario</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">     idx: indice de la palabra </span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: tamaño del vocabulario</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">    np.array de lagro "vocab_size"</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init array encodeado</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    one_hot <span class="op">=</span> np.zeros(vocab_size)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setamos el elemento a uno</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    one_hot[idx] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> one_hot</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode_sequence(sequence, vocab_size):</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Encodea una secuencia de palabras dado el tamaño del vocabulario</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co">     sentence: una lista de palabras a encodear</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: tamaño del vocabulario</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co">     </span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">    np.array 3D de tamaño (numero de palabras, vocab_size, 1)</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encodear cada palabra en la secuencia</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> np.array([one_hot_encode(word_to_idx[word], vocab_size) <span class="cf">for</span> word <span class="kw">in</span> sequence])</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cambiar de forma para tener (num words, vocab size, 1)</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    encoding <span class="op">=</span> encoding.reshape(encoding.shape[<span class="dv">0</span>], encoding.shape[<span class="dv">1</span>], <span class="dv">1</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoding</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>test_word <span class="op">=</span> one_hot_encode(word_to_idx[<span class="st">'a'</span>], vocab_size)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encodeado de 'a' con forma </span><span class="sc">{</span>test_word<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>test_sentence <span class="op">=</span> one_hot_encode_sequence([<span class="st">'a'</span>, <span class="st">'b'</span>], vocab_size)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encodeado de la secuencia 'a b' con forma </span><span class="sc">{</span>test_sentence<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Encodeado de 'a' con forma (4,)
Encodeado de la secuencia 'a b' con forma (2, 4, 1).</code></pre>
</div>
</div>
<p>Ahora que ya tenemos lo necesario de data para empezar a trabajar, demos paso a hablar un poco más de las RNN</p>
</section>
</section>
<section id="redes-neuronales-recurrentes-rnn" class="level2">
<h2 class="anchored" data-anchor-id="redes-neuronales-recurrentes-rnn">Redes Neuronales Recurrentes (RNN)</h2>
<p>Una red neuronal recurrente (RNN) es una red neuronal conocida por modelar de manera efectiva datos secuenciales como el lenguaje, el habla y las secuencias de proteínas. Procesa datos de manera cíclica, aplicando los mismos cálculos a cada elemento de una secuencia. Este enfoque cíclico permite que la red utilice cálculos anteriores como una forma de memoria, lo que ayuda a hacer predicciones para cálculos futuros. Para comprender mejor este concepto, consideren la siguiente imagen.</p>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230518134831/What-is-Recurrent-Neural-Network.webp" alt="RNN"></p>
<p><em>Crédito de imagen al autor, imagen tomada de “Introduction to Recurrent Neural Network” de Aishwarya.27</em></p>
<p>Donde: * <span class="math inline">\(x\)</span> es la secuencia de input * <span class="math inline">\(U\)</span> es una matriz de pesos aplicada a una muestra de input dada * <span class="math inline">\(V\)</span> es una matriz de pesos usada para la computación recurrente para pasar la memroia en las secuencias * <span class="math inline">\(W\)</span> es una matriz de pesos usada para calcular la salida de cada paso * <span class="math inline">\(h\)</span> es el estado oculto (hidden state) (memoria de la red) para cada paso * <span class="math inline">\(L\)</span> es la salida resultante</p>
<p>Cuando una red es extendida como se muestra, es más facil referirse a un paso <span class="math inline">\(t\)</span>. Tenemos los siguientes calculos en la red</p>
<ul>
<li><span class="math inline">\(h_t=f(U x_t + V h_{t-1}\)</span> donde f es la función de activacion</li>
<li><span class="math inline">\(L_t = softmax(W h_t)\)</span></li>
</ul>
<section id="implementando-una-rnn" class="level3">
<h3 class="anchored" data-anchor-id="implementando-una-rnn">Implementando una RNN</h3>
<p>Ahora pasaremos a inicializar nuestra RNN. Los pesos suelen inicializar de forma aleatoria, pero esta vez lo haremos de forma ortogonal para mejorar el rendimiento de nuestra red, y siguiendo las recomendaciones del paper dado abajo.</p>
<p>Tenga cuidado al definir los elementos que se le piden, debido a que una mala dimensión causará que tenga resultados diferentes y errores al operar.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.139269Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.124344Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7aab983af86e5257de37bcca64632cee&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8c9797de901a1f19&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="41">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">50</span> <span class="co"># Numero de dimensiones en el hidden state</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>vocab_size  <span class="op">=</span> <span class="bu">len</span>(word_to_idx) <span class="co"># Tamaño del vocabulario</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_orthogonal(param):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Initializes weight parameters orthogonally.</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Inicializa los pesos ortogonalmente</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Esta inicialización está dada por el siguiente paper:</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">    https://arxiv.org/abs/1312.6120</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.ndim <span class="op">&lt;</span> <span class="dv">2</span>:</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Only parameters with 2 or more dimensions are supported."</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    rows, cols <span class="op">=</span> param.shape</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    new_param <span class="op">=</span> np.random.randn(rows, cols)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rows <span class="op">&lt;</span> cols:</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        new_param <span class="op">=</span> new_param.T</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular factorización QR</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    q, r <span class="op">=</span> np.linalg.qr(new_param)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hacer Q uniforme de acuerdo a https://arxiv.org/pdf/math-ph/0609050.pdf</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> np.diag(r, <span class="dv">0</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    ph <span class="op">=</span> np.sign(d)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    q <span class="op">*=</span> ph</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rows <span class="op">&lt;</span> cols:</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.T</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    new_param <span class="op">=</span> q</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_param</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_rnn(hidden_size, vocab_size):</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Inicializa la RNN</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_size:  Dimensiones del hidden state</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: Dimensión del vocabulario</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 5 lineas para </span></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos (input del hidden state)</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># U = </span></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos de los calculos recurrentes</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># V = </span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definir la matriz de pesos del hidden state a la salida</span></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W = </span></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias del hidden state</span></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b_hidden = </span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias de la salida</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b_out = </span></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para estas use np.zeros y asegurese de darle las dimensiones correcta a cada elemento</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> np.zeros((hidden_size, vocab_size))</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((hidden_size, hidden_size))</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.zeros((vocab_size, hidden_size))</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>    b_hidden <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>    b_out <span class="op">=</span> np.zeros((vocab_size, <span class="dv">1</span>))</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 3 lineas para inicializar los pesos de forma ortogonal usando la</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># funcion init_orthogonal</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># U =</span></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># V = </span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W = </span></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    U <span class="op">=</span> init_orthogonal(U)</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> init_orthogonal(V)</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> init_orthogonal(W)</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return parameters as a tuple</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> U, V, W, b_hidden, b_out</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_rnn(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.154917Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.142253Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5e16945840146775df25b57cf819b925&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-cebf0e26f26abbf2&quot;,&quot;locked&quot;:true,&quot;points&quot;:20,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="42">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">4</span>), <span class="fl">80.24369675632171</span>))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">50</span>), <span class="fl">3333.838548574836</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">2</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">80.6410290517092</span>))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">3</span>], ((<span class="dv">50</span>, <span class="dv">1</span>), <span class="fl">0.0</span>))</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">4</span>], ((<span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">0.0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<section id="funciones-de-activación" class="level4">
<h4 class="anchored" data-anchor-id="funciones-de-activación">Funciones de Activación</h4>
<p>A continuación definiremos las funciones de activación a usar, sigmoide, tanh y softmax.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.170217Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.156436Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a8ce75b321c0cc6ca5c2e37786a296f6&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-cda959974e86198a&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="43">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función sigmoide para un array x</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea sobre x_safe para implementar la funcion</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># f =</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x_safe))</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> f)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función tanh para un array x</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea sobre x_safe para implementar la funcion</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># f =</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> (np.exp(x_safe)<span class="op">-</span>np.exp(<span class="op">-</span>x_safe))<span class="op">/</span>(np.exp(x_safe)<span class="op">+</span>np.exp(<span class="op">-</span>x_safe))</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>f<span class="op">**</span><span class="dv">2</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x, derivative<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula la función softmax para un array x</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="co">     x: El array sobre el que trabajar</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="co">     derivative: Si esta como verdadero, regresar el valor en la derivada</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    x_safe <span class="op">=</span> x <span class="op">+</span> <span class="fl">1e-12</span> <span class="co">#Evitar ceros</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea sobre x_safe para implementar la funcion</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># f =</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.exp(x_safe)<span class="op">/</span>np.<span class="bu">sum</span>(np.exp(x_safe))</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa la derivada de la funcion</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> derivative: </span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span> <span class="co"># No se necesita en backprog</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regresa el valor para el paso forward</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.185565Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.172220Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;88b15c243905bba412ed5b4ba65b5be0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-a2ca064c7c460245&quot;,&quot;locked&quot;:true,&quot;points&quot;:15,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="44">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(sigmoid(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="fl">6.997641543410888</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(tanh(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="op">-</span><span class="fl">0.007401604025076086</span>))</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(softmax(params[<span class="dv">0</span>][<span class="dv">0</span>]), ((<span class="dv">4</span>,), <span class="fl">3.504688021096135</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="implementación-del-paso-forward" class="level4">
<h4 class="anchored" data-anchor-id="implementación-del-paso-forward">Implementación del paso Forward</h4>
<p>Ahora es el momento de implementar el paso forward usando lo que hemos implementado hasta ahora</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.200990Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.187073Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;65fdf4e2be5d9227b721ebfba3a76b88&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-d8f4885a4cccd525&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="45">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(inputs, hidden_state, params):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula el paso forward de RNN</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">     inputs: Seccuencia de input a ser procesada</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_state: Un estado inicializado hidden state</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">     params: Parametros de la RNN</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener los parametros</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    U, V, W, b_hidden, b_out <span class="op">=</span> params</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear una lista para guardar las salidas y los hidden states</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    outputs, hidden_states <span class="op">=</span> [], []</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada elemento en la secuencia input</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(inputs)):</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del nuevo hidden state usando tanh</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Recuerden que al ser el hidden state tienen que usar los pesos del input multiplicado por el input</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  a esto sumarle los pesos recurrentes por el hidden state y finalmente sumarle b</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden_state =</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> np.tanh(np.dot(U, inputs[t]) <span class="op">+</span> np.dot(V, hidden_state) <span class="op">+</span> b_hidden)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># para el calculo del output</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Al ser la salida, deben usar softmax sobre la multiplicación de pesos de salida con el hidden_state actual</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   es decir el calculado en el paso anterior y siempre sumarle su bias correspondiente</span></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out = </span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> softmax(np.dot(W, hidden_state) <span class="op">+</span> b_out)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Guardamos los resultados y continuamos</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        outputs.append(out)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>        hidden_states.append(hidden_state.copy())</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs, hidden_states</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.216845Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.202985Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b6b30539fff48162b40bf58b4d04a611&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-9db576244efaba24&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="46">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>test_input_sequence, test_target_sequence <span class="op">=</span> training_set[<span class="dv">0</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode </span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> one_hot_encode_sequence(test_input_sequence, vocab_size)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>test_target <span class="op">=</span> one_hot_encode_sequence(test_target_sequence, vocab_size)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state con zeros</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>outputs, hidden_states <span class="op">=</span> forward_pass(test_input, hidden_state, params)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Input:"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_input_sequence)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Target:"</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_target_sequence)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Predicha:"</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(outputs, ((<span class="dv">16</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">519.7419046193046</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Secuencia Input:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']
Secuencia Target:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']
Secuencia Predicha:
['a', 'b', 'a', 'a', 'a', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'b', 'b', 'b', 'b']</code></pre>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="implementación-del-paso-backward" class="level4">
<h4 class="anchored" data-anchor-id="implementación-del-paso-backward">Implementación del paso Backward</h4>
<p>Ahora es momento de implementar el paso backward. Si se pierden, remitanse a las ecuaciones e imagen dadas previamente.</p>
<p>Usaremos una función auxiliar para evitar la explición del gradiente. Esta tecnica suele funcionar muy bien, si quieren leer más sobre esto pueden consultar estos enlances</p>
<p><a href="https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem">Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)</a></p>
<p><a href="https://ai.stackexchange.com/questions/31991/what-exactly-happens-in-gradient-clipping-by-norm">What exactly happens in gradient clipping by norm?</a></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.232009Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.217875Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;7357465e411ae111b649d95e4fd7d6eb&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-9c36e2544990bfd5&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="47">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clip_gradient_norm(grads, max_norm<span class="op">=</span><span class="fl">0.25</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Clipea (recorta?) el gradiente para tener una norma máxima de `max_norm`</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Esto ayudará a prevenir el problema de la gradiente explosiva (BOOM!)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span> </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setea el máximo de la norma para que sea flotante</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    max_norm <span class="op">=</span> <span class="bu">float</span>(max_norm)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    total_norm <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculamos la norma L2 al cuadrado para cada gradiente y agregamos estas a la norma total</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> grad <span class="kw">in</span> grads:</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        grad_norm <span class="op">=</span> np.<span class="bu">sum</span>(np.power(grad, <span class="dv">2</span>))</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        total_norm <span class="op">+=</span> grad_norm</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cuadrado de la normal total</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    total_norm <span class="op">=</span> np.sqrt(total_norm)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculamos el coeficiente de recorte</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    clip_coef <span class="op">=</span> max_norm <span class="op">/</span> (total_norm <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Si el total de la norma es más grande que el máximo permitido, se recorta la gradiente</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> clip_coef <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> grad <span class="kw">in</span> grads:</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            grad <span class="op">*=</span> clip_coef</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grads</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward_pass(inputs, outputs, hidden_states, targets, params):</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Calcula el paso backward de la RNN</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co">     inputs: secuencia de input</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="co">     outputs: secuencia de output del forward</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_states: secuencia de los hidden_state del forward</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="co">     targets: secuencia target</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="co">     params: parametros de la RNN</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener los parametros</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    U, V, W, b_hidden, b_out <span class="op">=</span> params</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inicializamos las gradientes como cero (Noten que lo hacemos para los pesos y bias)</span></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    d_U, d_V, d_W <span class="op">=</span> np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    d_b_hidden, d_b_out <span class="op">=</span> np.zeros_like(b_hidden), np.zeros_like(b_out)</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Llevar el record de las derivadas de los hidden state y las perdidas (loss)</span></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>    d_h_next <span class="op">=</span> np.zeros_like(hidden_states[<span class="dv">0</span>])</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteramos para cada elemento en la secuencia output</span></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NB: Iteramos de regreso sobre t=N hasta 0</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(outputs))):</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida cross-entry (un escalar)</span></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Sumen +1e-12 a cada output_t</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint2: Recuerden que la perdida es el promedio de multiplicar el logaritmo de los output con los targets</span></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss +=</span></span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>np.mean(np.log(outputs[t]<span class="op">+</span><span class="fl">1e-12</span>) <span class="op">*</span> targets[t])</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>        d_o <span class="op">=</span> outputs[t].copy()</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para backpropagate en los output (derivada del cross-entropy)</span></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Si se sienten perdidos refieran a esta lectura: http://cs231n.github.io/neural-networks-case-study/#grad</span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_o[...] -=</span></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>        d_o[np.argmax(targets[t])] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_o[t] = outputs[t] - targets[t]</span></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 lineas para hacer el backpropagation de W</span></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_W += np.dot(...)</span></span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>        d_W <span class="op">+=</span> np.dot(d_o, hidden_states[t].T)</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>        d_b_out <span class="op">+=</span> d_o</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para hacer el backprop de h</span></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_h = </span></span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Probablemente necesiten sacar la transpuesta de W</span></span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint2: Recuerden sumar el bias correcto!</span></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>        d_h <span class="op">=</span> np.dot(W.T, d_o) <span class="op">+</span> d_h_next</span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular el backprop en la funcion de activacion tanh</span></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_f = </span></span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Recuerden pasar el parametro derivate=True a la funcion que definimos</span></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint2: Deben multiplicar con d_h</span></span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>        d_f <span class="op">=</span> tanh(hidden_states[t], derivative<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> d_h</span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>        d_b_hidden <span class="op">+=</span> d_f</span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para backprop en U</span></span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_U +=</span></span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a>        d_U <span class="op">+=</span> np.dot(d_f, inputs[t].T)</span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para backprop V</span></span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># d_V +=</span></span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a>        d_V <span class="op">+=</span> np.dot(d_f, hidden_states[t<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a>        d_h_next <span class="op">=</span> np.dot(V.T, d_f)</span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Empaquetar las gradientes</span></span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> d_U, d_V, d_W, d_b_hidden, d_b_out    </span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Corte de gradientes</span></span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> clip_gradient_norm(grads)</span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.248222Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.234016Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e114a2a7bf6752fd90bf75a740001356&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-65758aa67361b673&quot;,&quot;locked&quot;:true,&quot;points&quot;:20,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="48">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>loss, grads <span class="op">=</span> backward_pass(test_input, outputs, hidden_states, test_target, params)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_scalar(loss, <span class="st">'0xf0c8ccc9'</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">4</span>), <span class="op">-</span><span class="fl">16.16536590645467</span>))</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">155.12594909703253</span>))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(grads[<span class="dv">2</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="fl">1.5957812992239038</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="optimización" class="level4">
<h4 class="anchored" data-anchor-id="optimización">Optimización</h4>
<p>Considerando que ya tenemos el paso forward y podemos calcular gradientes con el backpropagation, ya podemos pasar a entrenar nuestra red. Para esto necesitaremos un optimizador. Una forma común y sencilla es implementar la gradiente descediente. Recuerden la regla de optimizacion <span class="math display">\[
θ = θ - α * ∇J(θ)
\]</span></p>
<ul>
<li><span class="math inline">\(θ\)</span> son los parametros del modelo</li>
<li><span class="math inline">\(α\)</span> es el learning rate</li>
<li><span class="math inline">\(∇J(θ)\)</span> representa la gradiente del costo J con respecto de los parametros</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:26:39.263752Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.249733Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;a816758f7791729583e774286d7ab13f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-54add6e82ed32f01&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="49">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">1e-3</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteramos sobre los parametros y las gradientes</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param, grad <span class="kw">in</span> <span class="bu">zip</span>(params, grads):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        param <span class="op">-=</span> lr <span class="op">*</span> grad</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="entrenamiento" class="level4">
<h4 class="anchored" data-anchor-id="entrenamiento">Entrenamiento</h4>
<p>Debemos establecer un ciclo de entrenamiento completo que involucre un paso forward, un paso backprop, un paso de optimización y validación. Se espera que el proceso de training dure aproximadamente 5 minutos (o menos), lo que le brinda la oportunidad de continuar leyendo mientras se ejecuta😜</p>
<p>Noten que estaremos viendo la perdida en el de validación (no en el de testing) esto se suele hacer para ir observando que tan bien va comportandose el modelo en terminos de generalización. Muchas veces es más recomendable ir viendo como evoluciona la métrica de desempeño principal (accuracy, recall, etc).</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.140050Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:26:39.266753Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;153355a9b2d4580f952a5a88a18762bf&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-e184f5f494d827a1&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="50">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametro</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva RNN</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_rnn(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hiddent state con ceros</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Rastreo de perdida (loss) para training y validacion</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos para cada epoca</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas en zero</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el grupo de validación</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el input y el target</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Re-init el hidden state</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> np.zeros_like(hidden_state)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso forward </span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs, hidden_states =</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>        outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso backward</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss, _ =</span></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>        loss, _ <span class="op">=</span> backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de perdida</span></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each sentence in training set</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el input y el target</span></span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Re-init el hidden state</span></span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> np.zeros_like(hidden_state)</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso forward </span></span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs, hidden_states = </span></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>        outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 line para el paso backward</span></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss, grads = </span></span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params)</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validar si la perdida es nan, llegamos al problema del vanishing gradient POOF! </span></span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.isnan(loss):</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"La gradiente se desvanecio... POOF!"</span>)</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de parámetros</span></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de perdida</span></span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para graficar</span></span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 100 epocas</span></span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoca </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoca 0, training loss: 4.050465094962605, validation loss: 4.801971835963545</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoca 100, training loss: 2.729834076572525, validation loss: 3.2320576163950805
Epoca 200, training loss: 2.1094146557357414, validation loss: 2.49805263288303
Epoca 300, training loss: 1.8235746981408254, validation loss: 2.198677070983756
Epoca 400, training loss: 1.6884087861994714, validation loss: 2.0770786080230583
Epoca 500, training loss: 1.6129170568125084, validation loss: 2.016354394171382
Epoca 600, training loss: 1.5624028954061135, validation loss: 1.9780311638490076
Epoca 700, training loss: 1.5235019197916626, validation loss: 1.94961304678416
Epoca 800, training loss: 1.48958280312922, validation loss: 1.9248315278144605
Epoca 900, training loss: 1.4558865884072298, validation loss: 1.8978220912154284
Epoca 1000, training loss: 1.4173709332617037, validation loss: 1.8600798176557602
Epoca 1100, training loss: 1.3681783634408253, validation loss: 1.799369702642105
Epoca 1200, training loss: 1.3051122158826352, validation loss: 1.7081695076518126
Epoca 1300, training loss: 1.2330985128135699, validation loss: 1.5999314734412224
Epoca 1400, training loss: 1.1619900522551039, validation loss: 1.4998577602412388
Epoca 1500, training loss: 1.1035554777978585, validation loss: 1.4282638416135263
Epoca 1600, training loss: 1.068063341629561, validation loss: 1.3958745915895114
Epoca 1700, training loss: 1.0550402179575662, validation loss: 1.3963674481781811
Epoca 1800, training loss: 1.057011100190773, validation loss: 1.4185760443882356
Epoca 1900, training loss: 1.0640880623589557, validation loss: 1.4524183517087312</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.294101Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.141603Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;9da3dd102d85550608abf1579a85656f&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-67387da31438dd57&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="51">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Veamos la primera secuencia en el test set</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init el hidden state con ceros</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hacemos el pase forward para evalular nuestra secuencia</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>outputs, hidden_states <span class="op">=</span> forward_pass(inputs_one_hot, hidden_state, params)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>output_sentence <span class="op">=</span> [idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Input:"</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Target:"</span>)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Predicha:"</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos la perdida</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Training loss'</span>,)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Validation loss'</span>)</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>), plt.ylabel(<span class="st">'NLL'</span>)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">10</span>):        </span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(outputs, ((<span class="dv">22</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">967.3924585924929</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Secuencia Input:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']
Secuencia Target:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']
Secuencia Predicha:
['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS']</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"10"}--> 
         ✓ [10 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="preguntas" class="level4">
<h4 class="anchored" data-anchor-id="preguntas">Preguntas</h4>
<p>Ya hemos visto el funcionamiento general de nuestra red RNN, viendo las gráficas de arriba, <strong>responda</strong> lo siguiente dentro de esta celda</p>
<ul>
<li><p>¿Qué interpretación le da a la separación de las graficas de training y validation?</p></li>
<li><p>Al observar las gráficas de training y validation, se puede ver que el modelo se ajusta muy bien a los datos de entrenamiento, pero no tanto a los datos de validación. Esto se debe a que el modelo se está sobreajustando a los datos de entrenamiento, por lo que no es capaz de generalizar bien a los datos de validación.</p></li>
<li><p>¿Cree que es un buen modelo basado solamente en el loss?</p></li>
<li><p>Al observar el loss, se puede concluir que el modelo es bueno. Esto se debe a que miesntras mayor cantidad de epocas se hagan mayor estabilidad va obteniendo el loss.</p></li>
<li><p>¿Cómo deberían de verse esas gráficas en un modelo ideal?</p></li>
<li><p>Para un modelo ideal, las graficas deberian de mostrarse lo más cercana posible, es decir, que el loss de training y validation sean muy similares. Sin que haya una diferencia como la que se puede ver en la grafica actual porque se sobreajusta a los datos de entrenamiento.</p></li>
</ul>
</section>
</section>
</section>
<section id="parte-2---construyendo-una-red-neuronal-lstm" class="level2">
<h2 class="anchored" data-anchor-id="parte-2---construyendo-una-red-neuronal-lstm">Parte 2 - Construyendo una Red Neuronal LSTM</h2>
<p><strong>Créditos:</strong> La segunda parte de este laboratorio está tomado y basado en uno de los laboratorios dados dentro del curso de “Deep Learning” de Jes Frellsen (DeepLearningDTU)</p>
<p>Consideren leer el siguiente blog para mejorar el entendimiento de este tema: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p>
<p>La RNN estándar enfrenta un problema de gradientes que desaparecen, lo que dificulta la retención de memoria en secuencias más largas. Para hacer frente a estos desafíos, se introdujeron algunas variantes.</p>
<p>Los dos tipos principales son la celda de memoria a corto plazo (LSTM) y la unidad recurrente cerrada (GRU), las cuales demuestran una capacidad mejorada para conservar y utilizar la memoria en pasos de tiempo posteriores.</p>
<p>En este ejercicio, nuestro enfoque estará en LSTM, pero los principios aprendidos aquí también se pueden aplicar fácilmente para implementar GRU.</p>
<p>Recordemos una de las imagenes que vimos en clase</p>
<p><img src="https://www.researchgate.net/profile/Savvas-Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg" alt="LSTM"></p>
<p><em>Crédito de imagen al autor, imagen tomada de “Designing neural network based decoders for surface codes” de Savvas Varsamopoulos</em></p>
<p>Recordemos que la “celula” de LST contiene tres tipos de gates, input, forget y output gate. La salida de una unidad LSTM está calculada por las siguientes funciones, donde <span class="math inline">\(\sigma = softmax\)</span>. Entonces tenemos la input gate <span class="math inline">\(i\)</span>, la forget gate <span class="math inline">\(f\)</span> y la output gate <span class="math inline">\(o\)</span></p>
<ul>
<li><span class="math inline">\(i = \sigma ( W^i [h_{t-1}, x_t])\)</span></li>
<li><span class="math inline">\(f = \sigma ( W^f [h_{t-1},x_t])\)</span></li>
<li><span class="math inline">\(o = \sigma ( W^o [h_{t-1},x_t])\)</span></li>
</ul>
<p>Donde <span class="math inline">\(W^i, W^f, W^o\)</span> son las matrices de pesos aplicada a cada aplicadas a una matriz contatenada <span class="math inline">\(h_{t-1}\)</span> (hidden state vector) y <span class="math inline">\(x_t\)</span> (input vector) para cada respectiva gate <span class="math inline">\(h_{t-1}\)</span>, del paso previo junto con el input actual <span class="math inline">\(x_t\)</span> son usados para calcular una memoria candidata <span class="math inline">\(g\)</span></p>
<ul>
<li><span class="math inline">\(g = tanh( W^g [h_{t-1}, x_t])\)</span></li>
</ul>
<p>El valor de la memoria <span class="math inline">\(c_t\)</span> es actualizada como</p>
<p><span class="math inline">\(c_t = c_{t-1} \circ f + g \circ i\)</span></p>
<p>donde <span class="math inline">\(c_{t-1}\)</span> es la memoria previa, y <span class="math inline">\(\circ\)</span> es una multiplicacion element-wise (recuerden que este tipo de multiplicación en numpy es con *)</p>
<p>La salida <span class="math inline">\(h_t\)</span> es calculada como</p>
<p><span class="math inline">\(h_t = tanh(c_t) \circ o\)</span></p>
<p>y este se usa para tanto la salida del paso como para el siguiente paso, mientras <span class="math inline">\(c_t\)</span> es exclusivamente enviado al siguiente paso. Esto hace <span class="math inline">\(c_t\)</span> una memoria feature, y no es usado directamente para caluclar la salida del paso actual.</p>
<section id="iniciando-una-red-lstm" class="level3">
<h3 class="anchored" data-anchor-id="iniciando-una-red-lstm">Iniciando una Red LSTM</h3>
<p>De forma similar a lo que hemos hecho antes, necesitaremos implementar el paso forward, backward y un ciclo de entrenamiento. Pero ahora usaremos LSTM con NumPy. Más adelante veremos como es que esto funciona con PyTorch.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.309450Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.296147Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;62b5aae14a3dc0ee3dbca646ce607e19&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-07f509efcc1a3ccb&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="52">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Tamaño del hidden state concatenado más el input</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>z_size <span class="op">=</span> hidden_size <span class="op">+</span> vocab_size </span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_lstm(hidden_size, vocab_size, z_size):</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Initializes our LSTM network.</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Init LSTM</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">     hidden_size: Dimensiones del hidden state</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co">     vocab_size: Dimensiones de nuestro vocabulario</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co">     z_size: Dimensiones del input concatenado </span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos de la forget gate</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_f = np.random.randn</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    W_f <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias del forget gate</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    b_f <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos de la input gate</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    W_i <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para input gate</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    b_i <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos para la memoria candidata</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recuerden que esta debe empezar con numeros aleatorios</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    W_g <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para la memoria candidata</span></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    b_g <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz de pesos para la output gate</span></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    W_o <span class="op">=</span> np.random.randn(hidden_size, z_size)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias para la output gate</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>    b_o <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aprox 1 linea para empezar la matriz que relaciona el hidden state con el output</span></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>    W_v <span class="op">=</span> np.random.randn(vocab_size, hidden_size)</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bias</span></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>    b_v <span class="op">=</span> np.zeros((vocab_size, <span class="dv">1</span>))</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init pesos ortogonalmente (https://arxiv.org/abs/1312.6120)</span></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>    W_f <span class="op">=</span> init_orthogonal(W_f)</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>    W_i <span class="op">=</span> init_orthogonal(W_i)</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>    W_g <span class="op">=</span> init_orthogonal(W_g)</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>    W_o <span class="op">=</span> init_orthogonal(W_o)</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>    W_v <span class="op">=</span> init_orthogonal(W_v)</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_lstm(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size, z_size<span class="op">=</span>z_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.325895Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.310447Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f54f80a804b45836347ca5928b1902b0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-1145b5a61bdcda0f&quot;,&quot;locked&quot;:true,&quot;points&quot;:25,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="53">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">0</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">28071.583543573637</span>))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">1</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">6337.520066952928</span>))</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">2</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="op">-</span><span class="fl">13445.986473992281</span>))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">3</span>], ((<span class="dv">50</span>, <span class="dv">54</span>), <span class="fl">2276.1116210911564</span>))</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(params[<span class="dv">4</span>], ((<span class="dv">4</span>, <span class="dv">50</span>), <span class="op">-</span><span class="fl">201.28961326044097</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="forward" class="level3">
<h3 class="anchored" data-anchor-id="forward">Forward</h3>
<p>Vamos para adelante con LSTM, al igual que previamente necesitamos implementar las funciones antes mencionadas</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.341418Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.326896Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b388082beee631c97ae27b131c638ee0&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-1277d0634231924c&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="54">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(inputs, h_prev, C_prev, p):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">    x: Input data en el paso "t", shape (n_x, m)</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co">    h_prev: Hidden state en el paso "t-1", shape (n_a, m)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co">    C_prev: Memoria en el paso "t-1", shape (n_a, m)</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">    p: Lista con pesos y biases, contiene:</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_f: Bias de la forget gate, shape (n_a, 1)</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_i: Bias de la update gate, shape (n_a, 1)</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_g: Pesos de la primer "tanh", shape (n_a, n_a + n_x)</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_g: Bias de la primer "tanh", shape (n_a, 1)</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_o: Bias de la output gate, shape (n_a, 1)</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co">    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s: Lista de tamaño m conteniendo los calculos de cada paso forward</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="co">    outputs: Predicciones en el paso "t", shape (n_v, m)</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validar las dimensiones</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> h_prev.shape <span class="op">==</span> (hidden_size, <span class="dv">1</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> C_prev.shape <span class="op">==</span> (hidden_size, <span class="dv">1</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Desempacar los parametros</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v <span class="op">=</span> p</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Listas para calculos de cada componente en LSTM</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    x_s, z_s, f_s, i_s,  <span class="op">=</span> [], [] ,[], []</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    g_s, C_s, o_s, h_s <span class="op">=</span> [], [] ,[], []</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    v_s, output_s <span class="op">=</span>  [], [] </span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Agregar los valores iniciales </span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    h_s.append(h_prev)</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>    C_s.append(C_prev)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> inputs:</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para concatenar el input y el hidden state</span></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z = np.row.stack(...)</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.row_stack((h_prev, x))</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>        z_s.append(z)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular el forget gate</span></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: recuerde usar sigmoid</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># f = </span></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> sigmoid(np.dot(W_f, z) <span class="op">+</span> b_f)</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>        f_s.append(f)</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del input gate</span></span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> sigmoid(np.dot(W_i, z) <span class="op">+</span> b_i)</span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a>        i_s.append(i)</span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la memoria candidata</span></span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> tanh(np.dot(W_g, z) <span class="op">+</span> b_g)</span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a>        g_s.append(g)</span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular el estado de la memoria</span></span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># C_prev = </span></span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>        C_prev <span class="op">=</span> f <span class="op">*</span> C_prev <span class="op">+</span> i <span class="op">*</span> g</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>        C_s.append(C_prev)</span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el calculo de la output gate</span></span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: recuerde usar sigmoid</span></span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># o = </span></span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> sigmoid(np.dot(W_o, z) <span class="op">+</span> b_o)</span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a>        o_s.append(o)</span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate hidden state</span></span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el calculo del hidden state</span></span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h_prev =</span></span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a>        h_prev <span class="op">=</span> o <span class="op">*</span> tanh(C_prev)</span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a>        h_s.append(h_prev)</span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calcular logits</span></span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.dot(W_v, h_prev) <span class="op">+</span> b_v</span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a>        v_s.append(v)</span>
<span id="cb35-85"><a href="#cb35-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-86"><a href="#cb35-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de output (con softmax)</span></span>
<span id="cb35-87"><a href="#cb35-87" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> softmax(v)</span>
<span id="cb35-88"><a href="#cb35-88" aria-hidden="true" tabindex="-1"></a>        output_s.append(output)</span>
<span id="cb35-89"><a href="#cb35-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-90"><a href="#cb35-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.356972Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.342416Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;94b91568cf22e1f75709bfe774316fd7&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-4c878e36c9c270ab&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="55">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia para probar</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode del input y target</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state con ceros</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>output_sentence <span class="op">=</span> [idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Input:"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Target:"</span>)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Predicha:"</span>)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> check_hash(outputs, ((<span class="dv">22</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fl">980.1651308051631</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Secuencia Input:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']
Secuencia Target:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']
Secuencia Predicha:
['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']</code></pre>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="backward" class="level3">
<h3 class="anchored" data-anchor-id="backward">Backward</h3>
<p>Ahora de reversa, al igual que lo hecho antes, necesitamos implementar el paso de backward</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.372456Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.358474Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;149234786a31e8903430dfe2ff9b25aa&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-8500a307f5192db0&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="56">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(z, f, i, g, C, o, h, v, outputs, targets, p <span class="op">=</span> params):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">    z: Input concatenado como una lista de tamaño m.</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">    f: Calculos del forget gate como una lista de tamaño m.</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">    i: Calculos del input gate como una lista de tamaño m.</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co">    g: Calculos de la memoria candidata como una lista de tamaño m.</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co">    C: Celdas estado como una lista de tamaño m+1.</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co">    o: Calculos del output gate como una lista de tamaño m.</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co">    h: Calculos del Hidden State como una lista de tamaño m+1.</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co">    v: Calculos del logit como una lista de tamaño m.</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a><span class="co">    outputs: Salidas como una lista de tamaño m.</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co">    targets: Targets como una lista de tamaño m.</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="co">    p: Lista con pesos y biases, contiene:</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_f:  Pesos de la forget gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_f: Bias de la forget gate, shape (n_a, 1)</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_i: Pesos de la update gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_i: Bias de la update gate, shape (n_a, 1)</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_g: Pesos de la primer "tanh", shape (n_a, n_a + n_x)</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_g: Bias de la primer "tanh", shape (n_a, 1)</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_o: Pesos de la output gate, shape (n_a, n_a + n_x)</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_o: Bias de la output gate, shape (n_a, 1)</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="co">                        W_v: Pesos de la matriz que relaciona el hidden state con el output, shape (n_v, n_a)</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="co">                        b_v: Bias que relaciona el hidden state con el output, shape (n_v, 1)</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="co">    loss: crossentropy loss para todos los elementos del output</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="co">    grads: lista de gradientes para todos los elementos en p</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Desempacar parametros</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v <span class="op">=</span> p</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Init gradientes con cero</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>    W_f_d <span class="op">=</span> np.zeros_like(W_f)</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>    b_f_d <span class="op">=</span> np.zeros_like(b_f)</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>    W_i_d <span class="op">=</span> np.zeros_like(W_i)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    b_i_d <span class="op">=</span> np.zeros_like(b_i)</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>    W_g_d <span class="op">=</span> np.zeros_like(W_g)</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>    b_g_d <span class="op">=</span> np.zeros_like(b_g)</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>    W_o_d <span class="op">=</span> np.zeros_like(W_o)</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>    b_o_d <span class="op">=</span> np.zeros_like(b_o)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>    W_v_d <span class="op">=</span> np.zeros_like(W_v)</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>    b_v_d <span class="op">=</span> np.zeros_like(b_v)</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setear la proxima unidad y hidden state con ceros</span></span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>    dh_next <span class="op">=</span> np.zeros_like(h[<span class="dv">0</span>])</span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>    dC_next <span class="op">=</span> np.zeros_like(C[<span class="dv">0</span>])</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para la perdida</span></span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iteramos en reversa los outputs</span></span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(outputs))):</span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida con cross entropy</span></span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss += ...</span></span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> <span class="op">-</span>np.mean(np.log(outputs[t]) <span class="op">*</span> targets[t])</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtener el hidden state del estado previo</span></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>        C_prev<span class="op">=</span> C[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivative of the relation of the hidden-state to the output gate</span></span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas en relacion del hidden state al output gate</span></span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>        dv <span class="op">=</span> np.copy(outputs[t])</span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a>        dv[np.argmax(targets[t])] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para actualizar la gradiente de la relacion del hidden-state al output gate</span></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># W_v_d += </span></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>        W_v_d <span class="op">+=</span> np.dot(dv, h[t].T)</span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a>        b_v_d <span class="op">+=</span> dv</span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la derivada del hidden state y el output gate</span></span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a>        dh <span class="op">=</span> np.dot(W_v.T, dv)        </span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a>        dh <span class="op">+=</span> dh_next</span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a>        do <span class="op">=</span> dh <span class="op">*</span> tanh(C[t])</span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la derivada del output</span></span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># do = ..</span></span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Recuerde multiplicar por el valor previo de do (el de arriba)</span></span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a>        do <span class="op">=</span> dh <span class="op">*</span> tanh(C[t]) <span class="op">*</span> sigmoid(o[t], derivative<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de las gradientes con respecto al output gate</span></span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a>        W_o_d <span class="op">+=</span> np.dot(do, z[t].T)</span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a>        b_o_d <span class="op">+=</span> do</span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del estado y la memoria candidata g</span></span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a>        dC <span class="op">=</span> np.copy(dC_next)</span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a>        dC <span class="op">+=</span> dh <span class="op">*</span> o[t] <span class="op">*</span> tanh(tanh(C[t]), derivative<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a>        dg <span class="op">=</span> dC <span class="op">*</span> i[t]</span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea de codigo para terminar el calculo de dg</span></span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a>        dg <span class="op">=</span> dC <span class="op">*</span> i[t] <span class="op">*</span> tanh(g[t], derivative<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de las gradientes con respecto de la mem candidata</span></span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a>        W_g_d <span class="op">+=</span> np.dot(dg, z[t].T)</span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a>        b_g_d <span class="op">+=</span> dg</span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-104"><a href="#cb38-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the derivative of the input gate and update its gradients</span></span>
<span id="cb38-105"><a href="#cb38-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de la derivada del input gate y la actualización de sus gradientes</span></span>
<span id="cb38-106"><a href="#cb38-106" aria-hidden="true" tabindex="-1"></a>        di <span class="op">=</span> dC <span class="op">*</span> g[t]</span>
<span id="cb38-107"><a href="#cb38-107" aria-hidden="true" tabindex="-1"></a>        di <span class="op">=</span> sigmoid(i[t], <span class="va">True</span>) <span class="op">*</span> di</span>
<span id="cb38-108"><a href="#cb38-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para el calculo de los pesos y bias del input gate</span></span>
<span id="cb38-109"><a href="#cb38-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># W_i_d += </span></span>
<span id="cb38-110"><a href="#cb38-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># b_i_d +=</span></span>
<span id="cb38-111"><a href="#cb38-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-112"><a href="#cb38-112" aria-hidden="true" tabindex="-1"></a>        W_i_d <span class="op">+=</span> np.dot(di, z[t].T)</span>
<span id="cb38-113"><a href="#cb38-113" aria-hidden="true" tabindex="-1"></a>        b_i_d <span class="op">+=</span> di</span>
<span id="cb38-114"><a href="#cb38-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-115"><a href="#cb38-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del forget gate y actualización de sus gradientes</span></span>
<span id="cb38-116"><a href="#cb38-116" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> dC <span class="op">*</span> C_prev</span>
<span id="cb38-117"><a href="#cb38-117" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> sigmoid(f[t]) <span class="op">*</span> df</span>
<span id="cb38-118"><a href="#cb38-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 2 lineas para el calculo de los pesos y bias de la forget gate</span></span>
<span id="cb38-119"><a href="#cb38-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># W_f_d += </span></span>
<span id="cb38-120"><a href="#cb38-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># b_f_d +=</span></span>
<span id="cb38-121"><a href="#cb38-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb38-122"><a href="#cb38-122" aria-hidden="true" tabindex="-1"></a>        W_f_d <span class="op">+=</span> np.dot(df, z[t].T)</span>
<span id="cb38-123"><a href="#cb38-123" aria-hidden="true" tabindex="-1"></a>        b_f_d <span class="op">+=</span> df</span>
<span id="cb38-124"><a href="#cb38-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-125"><a href="#cb38-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo de las derivadas del input y la actualizacion de gradientes del hidden state previo</span></span>
<span id="cb38-126"><a href="#cb38-126" aria-hidden="true" tabindex="-1"></a>        dz <span class="op">=</span> (np.dot(W_f.T, df)</span>
<span id="cb38-127"><a href="#cb38-127" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_i.T, di)</span>
<span id="cb38-128"><a href="#cb38-128" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_g.T, dg)</span>
<span id="cb38-129"><a href="#cb38-129" aria-hidden="true" tabindex="-1"></a>             <span class="op">+</span> np.dot(W_o.T, do))</span>
<span id="cb38-130"><a href="#cb38-130" aria-hidden="true" tabindex="-1"></a>        dh_prev <span class="op">=</span> dz[:hidden_size, :]</span>
<span id="cb38-131"><a href="#cb38-131" aria-hidden="true" tabindex="-1"></a>        dC_prev <span class="op">=</span> f[t] <span class="op">*</span> dC</span>
<span id="cb38-132"><a href="#cb38-132" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-133"><a href="#cb38-133" aria-hidden="true" tabindex="-1"></a>    grads<span class="op">=</span> W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d</span>
<span id="cb38-134"><a href="#cb38-134" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-135"><a href="#cb38-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Recorte de gradientes</span></span>
<span id="cb38-136"><a href="#cb38-136" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> clip_gradient_norm(grads)</span>
<span id="cb38-137"><a href="#cb38-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-138"><a href="#cb38-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, grads</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:29:37.388276Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.374451Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;f17904c9bbc54f6acdd9e59ead87adc0&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-baf03f239d56e288&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="57">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizamos un backward pass para probar</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>loss, grads <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perdida obtenida:</span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(check_scalar(loss, <span class="st">'0x53c34f25'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Perdida obtenida:7.637217940763248</code></pre>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
</section>
<section id="training" class="level3">
<h3 class="anchored" data-anchor-id="training">Training</h3>
<p>Ahora intentemos entrenar nuestro LSTM básico. Esta parte es muy similar a lo que ya hicimos previamente con la RNN</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:31:42.681386Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:29:37.389275Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1882170a6b982a00cd873c6d50cc1e09&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-cf9622776d252627&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="58">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametros</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva red</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>z_size <span class="op">=</span> hidden_size <span class="op">+</span> vocab_size <span class="co"># Tamaño del hidden concatenado + el input</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_lstm(hidden_size<span class="op">=</span>hidden_size, vocab_size<span class="op">=</span>vocab_size, z_size<span class="op">=</span>z_size)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state como ceros</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdida</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos cada epoca</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el validation set</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Init hidden state y la unidad de estado como ceros</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward</span></span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward </span></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        loss, _ <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el training set</span></span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>        targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Init hidden state y la unidad de estado como ceros</span></span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward</span></span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward</span></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)</span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualización de parametros</span></span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> update_parameters(params, grads, lr<span class="op">=</span><span class="fl">1e-1</span>)</span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss</span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para ser graficada</span></span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 5 epocas</span></span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, training loss: 2.988556571664096, validation loss: 4.499707061171418
Epoch 10, training loss: 1.2170995637244943, validation loss: 1.4488214228849636
Epoch 20, training loss: 0.9073644447199547, validation loss: 1.0815213281756293
Epoch 30, training loss: 0.9303750511396135, validation loss: 1.5909496802249132
Epoch 40, training loss: 0.9187082337270793, validation loss: 1.619079602264497
Epoch 50, training loss: 0.883855860180965, validation loss: 1.4990399686887048
Epoch 60, training loss: 0.8430567008576642, validation loss: 1.3609169236347676
Epoch 70, training loss: 0.8050372301595496, validation loss: 1.223916253362199
Epoch 80, training loss: 0.7809193343642156, validation loss: 1.1246054751802441
Epoch 90, training loss: 0.760033043780634, validation loss: 1.0526780777934726
Epoch 100, training loss: 0.7412121295781919, validation loss: 1.00791893577059
Epoch 110, training loss: 0.7254067392339139, validation loss: 0.9637842708291237
Epoch 120, training loss: 0.7202030582647296, validation loss: 0.9518831884975019
Epoch 130, training loss: 0.7194397177560573, validation loss: 0.9567955068844556
Epoch 140, training loss: 0.7154953247148214, validation loss: 0.9494748624138563
Epoch 150, training loss: 0.7088698499130716, validation loss: 0.9273671799359103
Epoch 160, training loss: 0.705925338281576, validation loss: 0.9148891658308586
Epoch 170, training loss: 0.7052924506358476, validation loss: 0.9135747466365054
Epoch 180, training loss: 0.6985401734929904, validation loss: 0.8918252304027119
Epoch 190, training loss: 0.6941816345156313, validation loss: 0.8758085908845666
Epoch 200, training loss: 0.6942684719433829, validation loss: 0.8791512120670207
Epoch 210, training loss: 0.6974052532459426, validation loss: 0.8973331347155851
Epoch 220, training loss: 0.7062945988288707, validation loss: 0.9384257964585396
Epoch 230, training loss: 0.7216060934816805, validation loss: 0.999996336113707
Epoch 240, training loss: 0.7373005079892263, validation loss: 1.0602555584469302
Epoch 250, training loss: 0.7497432590084389, validation loss: 1.1076059778533052
Epoch 260, training loss: 0.7584753648557375, validation loss: 1.1405620004978552
Epoch 270, training loss: 0.7630605106520527, validation loss: 1.1579921733498533
Epoch 280, training loss: 0.7628045874444377, validation loss: 1.1582162982098108
Epoch 290, training loss: 0.7574272019036931, validation loss: 1.1407415819565816
Epoch 300, training loss: 0.7480212844162684, validation loss: 1.1088936193127996
Epoch 310, training loss: 0.7379643670485312, validation loss: 1.072965995090395
Epoch 320, training loss: 0.7325371067455377, validation loss: 1.05021677207737
Epoch 330, training loss: 0.7358520224675805, validation loss: 1.0543313729472126
Epoch 340, training loss: 0.7495176769310687, validation loss: 1.088320591378109
Epoch 350, training loss: 0.7740690948156395, validation loss: 1.151057005358313
Epoch 360, training loss: 0.7943872549214672, validation loss: 1.201935472935011
Epoch 370, training loss: 0.7910334176853717, validation loss: 1.1866317937048139
Epoch 380, training loss: 0.7667557816129712, validation loss: 1.1200050465959706
Epoch 390, training loss: 0.7242461959740364, validation loss: 0.9978593449146912
Epoch 400, training loss: 0.6994646748068695, validation loss: 0.9090963464642516
Epoch 410, training loss: 0.7053350250070572, validation loss: 0.9251312017335562
Epoch 420, training loss: 0.7137177022967298, validation loss: 0.954699091400423
Epoch 430, training loss: 0.7174796634985487, validation loss: 0.9680080894806151
Epoch 440, training loss: 0.7187181648105163, validation loss: 0.9725416272600189
Epoch 450, training loss: 0.7182025550396641, validation loss: 0.9709316020010533
Epoch 460, training loss: 0.7164488026960367, validation loss: 0.9651077128274617
Epoch 470, training loss: 0.713620883784846, validation loss: 0.9558919777551713
Epoch 480, training loss: 0.7092569990230373, validation loss: 0.942009029340954
Epoch 490, training loss: 0.7020622408672974, validation loss: 0.919304694643067</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:31:42.819860Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:31:42.683278Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5db6b37684f2913ca50ec8a4c8f5981f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-7814184dd4823fac&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="59">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia del test set</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>targets_one_hot <span class="op">=</span> one_hot_encode_sequence(targets, vocab_size)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Init hidden state como ceros</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward </span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs <span class="op">=</span> forward(inputs_one_hot, h, c, params)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Input:"</span>)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Target:"</span>)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Predicha:"</span>)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([idx_to_word[np.argmax(output)] <span class="cf">for</span> output <span class="kw">in</span> outputs])</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar la perdida en training y validacion</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Training loss'</span>,)</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Validation loss'</span>)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>), plt.ylabel(<span class="st">'NLL'</span>)</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Secuencia Input:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']
Secuencia Target:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']
Secuencia Predicha:
['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
</div>
<section id="preguntas-1" class="level4">
<h4 class="anchored" data-anchor-id="preguntas-1">Preguntas</h4>
<p><strong>Responda</strong> lo siguiente dentro de esta celda</p>
<ul>
<li>¿Qué modelo funcionó mejor? ¿RNN tradicional o el basado en LSTM? ¿Por qué?</li>
<li>El modelo basado en LSTM funcionó mejor, ya que la LSTM tiene una memoria interna que le permite recordar patrones de largo plazo, mientras que la RNN tradicional no tiene memoria interna.</li>
<li>Observen la gráfica obtenida arriba, ¿en qué es diferente a la obtenida a RNN? ¿Es esto mejor o peor? ¿Por qué?</li>
<li>La gráfica obtenida con la RNN tradicional es más suave que la de LSTM. Sin embargo la gráfica de LSTM con una menor cantidad de epocas logra dar una perdida menor que la de RNN tradicional, por lo que se puede decir que es mejor.</li>
<li>¿Por qué LSTM puede funcionar mejor con secuencias largas?</li>
<li>Porque la LSTM tiene una memoria interna que le permite recordar patrones de largo plazo, esto presenta una ventaja ante la RNN tradicional.</li>
</ul>
</section>
</section>
</section>
<section id="parte-3---red-neuronal-lstm-con-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="parte-3---red-neuronal-lstm-con-pytorch">Parte 3 - Red Neuronal LSTM con PyTorch</h2>
<p>Ahora que ya hemos visto el funcionamiento paso a paso de tanto RNN tradicional como LSTM. Es momento de usar PyTorch. Para esta parte usaremos el mismo dataset generado al inicio. Así mismo, usaremos un ciclo de entrenamiento similar al que hemos usado previamente.</p>
<p>En la siguiente parte (sí, hay una siguiente parte 🤓) usaremos otro tipo de dataset más formal</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:31:42.835972Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:31:42.821945Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ee2d3fa1a4e9d2426203334a38a4af8e&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-311fc1fe42eca687&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="60">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1-3 lineas de codigo para declarar una capa LSTM</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.lstm = </span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Esta tiene que tener el input_size del tamaño del vocabulario,</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     debe tener 50 hidden states (hidden_size)</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     una layer</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     y NO (False) debe ser bidireccional </span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span>vocab_size,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>                            hidden_size<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>                            num_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>                            bidirectional<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer de salida (output)</span></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_out <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>                            out_features<span class="op">=</span>vocab_size,</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>                            bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RNN regresa el output y el ultimo hidden state</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>        x, (h, c) <span class="op">=</span> <span class="va">self</span>.lstm(x)</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aplanar la salida para una layer feed forward</span></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.lstm.hidden_size)</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># layer de output </span></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.l_out(x)</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(net)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Net(
  (lstm): LSTM(4, 50)
  (l_out): Linear(in_features=50, out_features=4, bias=False)
)</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:36.624191Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:31:42.837717Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;ad63c124dd865aa9b8c0da08852718ad&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-04486b8d9ade1533&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="61">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper parametros</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Init una nueva red</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 2 lineas para definir la función de perdida y el optimizador</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = # Use CrossEntropy</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = # Use Adam con lr=3e-4</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(net.parameters(), lr<span class="op">=</span><span class="fl">3e-4</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdida</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>training_loss, validation_loss <span class="op">=</span> [], []</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos cada epoca</span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perdidas</span></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    epoch_training_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    epoch_validation_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NOTA 1</span></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>    net.<span class="bu">eval</span>()</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el validation set</span></span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> validation_set:</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el input a un tensor</span></span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el target a un tensor</span></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs = </span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> net.forward(inputs_one_hot)</span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-46"><a href="#cb47-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida</span></span>
<span id="cb47-47"><a href="#cb47-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss =</span></span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Use el criterion definido arriba</span></span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, targets_idx)</span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a>        epoch_validation_loss <span class="op">+=</span> loss.detach().numpy()</span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># NOTA 2</span></span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a>    net.train()</span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para cada secuencia en el training set</span></span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> inputs, targets <span class="kw">in</span> training_set:</span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># One-hot encode el inpyt y el target</span></span>
<span id="cb47-62"><a href="#cb47-62" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb47-63"><a href="#cb47-63" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb47-64"><a href="#cb47-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-65"><a href="#cb47-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el input a un tensor</span></span>
<span id="cb47-66"><a href="#cb47-66" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb47-67"><a href="#cb47-67" aria-hidden="true" tabindex="-1"></a>        inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb47-68"><a href="#cb47-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-69"><a href="#cb47-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convertir el target a un tensor</span></span>
<span id="cb47-70"><a href="#cb47-70" aria-hidden="true" tabindex="-1"></a>        targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb47-71"><a href="#cb47-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-72"><a href="#cb47-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb47-73"><a href="#cb47-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># outputs = </span></span>
<span id="cb47-74"><a href="#cb47-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-75"><a href="#cb47-75" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> net.forward(inputs_one_hot)</span>
<span id="cb47-76"><a href="#cb47-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-77"><a href="#cb47-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 1 linea para calcular la perdida</span></span>
<span id="cb47-78"><a href="#cb47-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss =</span></span>
<span id="cb47-79"><a href="#cb47-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: Use el criterion definido arriba</span></span>
<span id="cb47-80"><a href="#cb47-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-81"><a href="#cb47-81" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, targets_idx)</span>
<span id="cb47-82"><a href="#cb47-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-83"><a href="#cb47-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aprox 3 lineas para definir el backward</span></span>
<span id="cb47-84"><a href="#cb47-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimizer.</span></span>
<span id="cb47-85"><a href="#cb47-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># loss.</span></span>
<span id="cb47-86"><a href="#cb47-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimizer.</span></span>
<span id="cb47-87"><a href="#cb47-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb47-88"><a href="#cb47-88" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb47-89"><a href="#cb47-89" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb47-90"><a href="#cb47-90" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb47-91"><a href="#cb47-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-92"><a href="#cb47-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actualizacion de la perdida</span></span>
<span id="cb47-93"><a href="#cb47-93" aria-hidden="true" tabindex="-1"></a>        epoch_training_loss <span class="op">+=</span> loss.detach().numpy()</span>
<span id="cb47-94"><a href="#cb47-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-95"><a href="#cb47-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guardar la perdida para ser graficada</span></span>
<span id="cb47-96"><a href="#cb47-96" aria-hidden="true" tabindex="-1"></a>    training_loss.append(epoch_training_loss<span class="op">/</span><span class="bu">len</span>(training_set))</span>
<span id="cb47-97"><a href="#cb47-97" aria-hidden="true" tabindex="-1"></a>    validation_loss.append(epoch_validation_loss<span class="op">/</span><span class="bu">len</span>(validation_set))</span>
<span id="cb47-98"><a href="#cb47-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-99"><a href="#cb47-99" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar la perdida cada 5 epocas</span></span>
<span id="cb47-100"><a href="#cb47-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb47-101"><a href="#cb47-101" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, training loss: </span><span class="sc">{</span>training_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">, validation loss: </span><span class="sc">{</span>validation_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, training loss: 1.332130877673626, validation loss: 1.4030044198036193
Epoch 10, training loss: 0.5665819689631462, validation loss: 0.5150988012552261
Epoch 20, training loss: 0.4065188027918339, validation loss: 0.3513254955410957
Epoch 30, training loss: 0.35732000395655633, validation loss: 0.31538560837507246
Epoch 40, training loss: 0.3318668494001031, validation loss: 0.2908444494009018
Epoch 50, training loss: 0.3165944604203105, validation loss: 0.2807041689753532
Epoch 60, training loss: 0.30732162185013295, validation loss: 0.2749412447214127
Epoch 70, training loss: 0.3016716230660677, validation loss: 0.27157292068004607
Epoch 80, training loss: 0.2981261992827058, validation loss: 0.2691588431596756
Epoch 90, training loss: 0.29602834302932024, validation loss: 0.26835395842790605
Epoch 100, training loss: 0.29472043365240097, validation loss: 0.26672435104846953
Epoch 110, training loss: 0.29391421396285294, validation loss: 0.26594287157058716
Epoch 120, training loss: 0.29325835723429916, validation loss: 0.26548516303300856
Epoch 130, training loss: 0.29329489823430777, validation loss: 0.27009081095457077
Epoch 140, training loss: 0.29195698667317627, validation loss: 0.26556223034858706
Epoch 150, training loss: 0.29179672673344614, validation loss: 0.2653796553611755
Epoch 160, training loss: 0.29161879867315293, validation loss: 0.26540517061948776
Epoch 170, training loss: 0.2914200507104397, validation loss: 0.26561194360256196
Epoch 180, training loss: 0.29121116288006305, validation loss: 0.26598367989063265
Epoch 190, training loss: 0.2908841595053673, validation loss: 0.2671591594815254
Epoch 200, training loss: 0.29045742433518174, validation loss: 0.2671342223882675
Epoch 210, training loss: 0.29039867743849757, validation loss: 0.2675590977072716
Epoch 220, training loss: 0.290312166698277, validation loss: 0.26827752143144606
Epoch 230, training loss: 0.29022215977311133, validation loss: 0.26914853155612944
Epoch 240, training loss: 0.29016127474606035, validation loss: 0.2699747934937477
Epoch 250, training loss: 0.2895416425541043, validation loss: 0.27109490931034086
Epoch 260, training loss: 0.28962583281099796, validation loss: 0.27112736403942106
Epoch 270, training loss: 0.28971136044710877, validation loss: 0.2712604030966759
Epoch 280, training loss: 0.28976680152118206, validation loss: 0.27147339582443236
Epoch 290, training loss: 0.28979758694767954, validation loss: 0.27170084714889525
Epoch 300, training loss: 0.2897938057780266, validation loss: 0.2718871757388115
Epoch 310, training loss: 0.28935408517718314, validation loss: 0.27224015444517136
Epoch 320, training loss: 0.2894421933218837, validation loss: 0.27225979417562485
Epoch 330, training loss: 0.28951069843024013, validation loss: 0.2723362535238266
Epoch 340, training loss: 0.28955185804516076, validation loss: 0.2724667593836784
Epoch 350, training loss: 0.28957198187708855, validation loss: 0.27263348549604416
Epoch 360, training loss: 0.3345987621694803, validation loss: 0.6096083223819733
Epoch 370, training loss: 0.28896120470017195, validation loss: 0.2733343422412872
Epoch 380, training loss: 0.2890737453475595, validation loss: 0.27312382459640505
Epoch 390, training loss: 0.2891850983723998, validation loss: 0.27303295731544497
Epoch 400, training loss: 0.28926859702914953, validation loss: 0.27305328398942946
Epoch 410, training loss: 0.28932365756481887, validation loss: 0.2731513649225235
Epoch 420, training loss: 0.28935381267219784, validation loss: 0.27330504953861234
Epoch 430, training loss: 0.289364630356431, validation loss: 0.27341592460870745
Epoch 440, training loss: 0.28936118017882106, validation loss: 0.2735031843185425
Epoch 450, training loss: 0.2889531644061208, validation loss: 0.2736992180347443
Epoch 460, training loss: 0.2890328589826822, validation loss: 0.27365635335445404
Epoch 470, training loss: 0.2890869822353125, validation loss: 0.2736848756670952
Epoch 480, training loss: 0.2891178911551833, validation loss: 0.27374170124530794
Epoch 490, training loss: 0.28912767488509417, validation loss: 0.2738093167543411</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:36.640178Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:36.626695Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;18a505ffb2aa6222c3894bc5fee82e02&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-acfe6153f9006b27&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="62">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(training_loss[<span class="op">-</span><span class="dv">1</span>]), <span class="st">"3c3d"</span>, <span class="st">'0x1.28f5c28f5c28fp-2'</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">5</span>):        </span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> compare_numbers(new_representation(validation_loss[<span class="op">-</span><span class="dv">1</span>]), <span class="st">"3c3d"</span>, <span class="st">'0x1.28f5c28f5c28fp-2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"5"}--> 
         ✓ [5 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:36.781648Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:36.642178Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5847ed5bbead7e432e5e12d4eb6114a3&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-3e1bfd6f4ff9568e&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="76">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener la primera secuencia del test set</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>inputs, targets <span class="op">=</span> test_set[<span class="dv">1</span>]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode el input y el target</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> one_hot_encode_sequence(inputs, vocab_size)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>targets_idx <span class="op">=</span> [word_to_idx[word] <span class="cf">for</span> word <span class="kw">in</span> targets]</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir el input a un tensor</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> torch.Tensor(inputs_one_hot)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>inputs_one_hot <span class="op">=</span> inputs_one_hot.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir el target a un tensor</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>targets_idx <span class="op">=</span> torch.LongTensor(targets_idx)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Aprox 1 linea para el Forward </span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="co"># outputs = </span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> net(inputs_one_hot)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Input:"</span>)</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs)</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Target:"</span>)</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(targets)</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Secuencia Predicha:"</span>)</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a><span class="co">#print([idx_to_word[np.argmax(output)] for output in outputs])</span></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar la perdida en training y validacion</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(training_loss))</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, training_loss, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Training loss'</span>,)</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, validation_loss, <span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Validation loss'</span>)</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>), plt.ylabel(<span class="st">'NLL'</span>)</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Secuencia Input:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']
Secuencia Target:
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']
Secuencia Predicha:</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-35-output-2.png" class="img-fluid"></p>
</div>
</div>
<section id="preguntas-2" class="level4">
<h4 class="anchored" data-anchor-id="preguntas-2">Preguntas</h4>
<p><strong>Responda</strong> lo siguiente dentro de esta celda</p>
<ul>
<li><p>Compare las graficas obtenidas en el LSTM “a mano” y el LSTM “usando PyTorch, ¿cuál cree que es mejor? ¿Por qué?</p></li>
<li><p>Al obervar las gráficas se puede concluir que el LSTM usando PyTorch es mejor, ya que la gráfica de la predicción se asemeja más a la gráfica de la función seno original. Esto se debe a que el LSTM usando PyTorch está altamente optimizado y puede aprovechar mejor los recursos.</p></li>
<li><p>Compare la secuencia target y la predicha de esta parte, ¿en qué parte falló el modelo?</p></li>
<li><p>En primer lugar el modelo su pudo haber sobreajustado, por lo que se adapto a las condiciones de los datos de entrenamiento, en lugar de aprender bien los patrones. Además de ello el modelo pudo requerir de mayor cantidad de epocas para que su entrenamiento fuera más óptimo.</p></li>
<li><p>¿Qué sucede en el código donde se señala “NOTA 1” y “NOTA 2”? ¿Para qué son necesarias estas líneas?</p></li>
<li><p>En la parte del NOTA 1 se establece en modo de evaluación (eval). Durante la fase de evaluación, el modelo se utiliza para hacer predicciones en nuevos datos, pero no se actualizan los pesos del modelo ni se calculan gradientes. Mientras que en la parte de NOTA 2 En esta línea, el modelo se establece en modo de entrenamiento (train). Durante la fase de entrenamiento, el modelo se ajusta a los datos y se actualizan los pesos utilizando el algoritmo de optimización.</p></li>
</ul>
</section>
</section>
<section id="parte-4---segunda-red-neuronal-lstm-con-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="parte-4---segunda-red-neuronal-lstm-con-pytorch">Parte 4 - Segunda Red Neuronal LSTM con PyTorch</h2>
<p>Para esta parte será un poco menos guiada, por lo que se espera que puedan generar un modelo de Red Neuronal con LSTM para solventar un problema simple. Lo que se evaluará es la métrica final, y solamente se dejarán las generalidades de la implementación. El objetivo de esta parte, es dejar que ustedes exploren e investiguen un poco más por su cuenta.</p>
<p>En este parte haremos uso de las redes LSTM pero para predicción de series de tiempo. Entonces lo que se busca es que dado un mes y un año, se debe predecir el número de pasajeros en unidades de miles. Los datos a usar son de 1949 a 1960.</p>
<p>Basado del blog “LSTM for Time Series Prediction in PyTorch” de Adrian Tam.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:36.796931Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:36.782645Z&quot;}" data-execution_count="64">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed all</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>random.seed(seed_)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed(seed_)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed_)  <span class="co"># Multi-GPU.</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:38.551864Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:36.798433Z&quot;}" data-execution_count="65">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>url_data <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> pd.read_csv(url_data)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>dataset.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Month</th>
      <th>Passengers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1949-01</td>
      <td>112</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1949-02</td>
      <td>118</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1949-03</td>
      <td>132</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1949-04</td>
      <td>129</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1949-05</td>
      <td>121</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1949-06</td>
      <td>135</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1949-07</td>
      <td>148</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1949-08</td>
      <td>148</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1949-09</td>
      <td>136</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1949-10</td>
      <td>119</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:38.662912Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:38.554181Z&quot;}" data-execution_count="66">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dibujemos la serie de tiempo</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>time_series <span class="op">=</span> dataset[[<span class="st">"Passengers"</span>]].values.astype(<span class="st">'float32'</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>plt.plot(time_series)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Esta serie de tiempo comprende 144 pasos de tiempo. El gráfico indica claramente una tendencia al alza y hay patrones periódicos en los datos que corresponden al período de vacaciones de verano. Por lo general, se recomienda “eliminar la tendencia” de la serie temporal eliminando el componente de tendencia lineal y normalizándolo antes de continuar con el procesamiento. Sin embargo, por simplicidad de este ejercicios, vamos a omitir estos pasos.</p>
<p>Ahora necesitamos dividir nuestro dataset en training, validation y test set. A diferencia de otro tipo de datasets, cuando se trabaja en este tipo de proyectos, la división se debe hacer sin “revolver” los datos. Para esto, podemos hacerlo con NumPy</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:38.678359Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:38.663917Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;07cb1e706347a5e56eac2633b37bcaf1&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-35af372f0bf820a2&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="67">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># En esta ocasion solo usaremos train y test, validation lo omitiremos para simpleza del ejercicio</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># NO CAMBIEN NADA DE ESTA CELDA POR FAVOR</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>p_train<span class="op">=</span><span class="fl">0.8</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>p_test<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Definimos el tamaño de las particiones</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>num_train <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(time_series)<span class="op">*</span>p_train)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>num_test <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(time_series)<span class="op">*</span>p_test)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir las secuencias en las particiones</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> time_series[:num_train]</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> time_series[num_train:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>El aspecto más complicado es determinar el método por el cual la red debe predecir la serie temporal. Por lo general, la predicción de series temporales se realiza en función de una ventana. En otras palabras, recibe datos del tiempo t1 al t2, y su tarea es predecir para el tiempo t3 (o más adelante). El tamaño de la ventana, denotado por w, dicta cuántos datos puede considerar el modelo al hacer la predicción. Este parámetro también se conoce como <strong>look back period</strong> (período retrospectivo).</p>
<p>Entonces, creemos una función para obtener estos datos, dado un look back period. Además, debemos asegurarnos de transformar estos datos a tensores para poder ser usados con PyTorch.</p>
<p>Esta función está diseñada para crear ventanas en la serie de tiempo mientras predice un paso de tiempo en el futuro inmediato. Su propósito es convertir una serie de tiempo en un tensor con dimensiones (muestras de ventana, pasos de tiempo, características). Dada una serie de tiempo con t pasos de tiempo, puede producir aproximadamente (t - ventana + 1) ventanas, donde “ventana” denota el tamaño de cada ventana. Estas ventanas pueden comenzar desde cualquier paso de tiempo dentro de la serie de tiempo, siempre que no se extiendan más allá de sus límites.</p>
<p>Cada ventana contiene múltiples pasos de tiempo consecutivos con sus valores correspondientes, y cada paso de tiempo puede tener múltiples características. Sin embargo, en este conjunto de datos específico, solo hay una función disponible.</p>
<p>La elección del diseño garantiza que tanto la “característica” como el “objetivo” tengan la misma forma. Por ejemplo, para una ventana de tres pasos de tiempo, la “característica” corresponde a la serie de tiempo de t-3 a t-1, y el “objetivo” cubre los pasos de tiempo de t-2 a t. Aunque estamos principalmente interesados en predecir t+1, la información de t-2 a t es valiosa durante el entrenamiento.</p>
<p>Es importante tener en cuenta que la serie temporal de entrada se representa como una matriz 2D, mientras que la salida de la función <code>create_timeseries_dataset()</code> será un tensor 3D. Para demostrarlo, usemos lookback=1 y verifiquemos la forma del tensor de salida en consecuencia.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:38.694314Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:38.680182Z&quot;}" data-execution_count="68">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_timeseries_dataset(dataset, lookback):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> [], []</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dataset) <span class="op">-</span> lookback):</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        feature <span class="op">=</span> dataset[i : i <span class="op">+</span> lookback]</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> dataset[i <span class="op">+</span> <span class="dv">1</span> : i <span class="op">+</span> lookback <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        X.append(feature)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        y.append(target)</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(X), torch.tensor(y)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co"># EL VALOR DE LB SÍ LO PUEDEN CAMBIAR SI LO CONSIDERAN NECESARIO</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>lb <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> create_timeseries_dataset(train, lookback<span class="op">=</span>lb)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co">#X_validation, y_validation = create_timeseries_dataset(validation, lookback=lb)</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> create_timeseries_dataset(test, lookback<span class="op">=</span>lb)</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, y_train.shape)</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a><span class="co">#print(X_validation.shape, y_validation.shape)</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_test.shape, y_test.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([111, 4, 1]) torch.Size([111, 4, 1])
torch.Size([25, 4, 1]) torch.Size([25, 4, 1])</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\diego\AppData\Local\Temp\ipykernel_22568\2018909527.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\torch\csrc\utils\tensor_new.cpp:248.)
  return torch.tensor(X), torch.tensor(y)</code></pre>
</div>
</div>
<p>Ahora necesitamos crear una clase que definirá nuestro modelo de red neuronal con LSTM. Noten que acá solo se dejaran las firmas de las funciones necesarias, ustedes deberán decidir que arquitectura con LSTM implementar, con la finalidad de superar cierto threshold de métrica de desempeño mencionado abajo.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:32:38.709484Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:38.702496Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;37d446d97ee24bb313f280d371544a3a&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-f0f68d3f484736df&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}" data-execution_count="69">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomModelLSTM(nn.Module):</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span><span class="dv">1</span>, hidden_size<span class="op">=</span><span class="dv">50</span>, num_layers<span class="op">=</span><span class="dv">1</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="va">self</span>.lstm(x)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La función nn.LSTM() produce una tupla como salida. El primer elemento de esta tupla consiste en los hidden states generados, donde cada paso de tiempo de la entrada tiene su correspondiente hidden state. El segundo elemento contiene la memoria y los hidden states de la unidad LSTM, pero no se usan en este contexto particular.</p>
<p>La capa LSTM se configura con la opción <code>batch_first=True</code> porque los tensores de entrada se preparan en la dimensión de (muestra de ventana, pasos de tiempo, características). Con esta configuración, se crea un batch tomando muestras a lo largo de la primera dimensión.</p>
<p>Para generar un único resultado de regresión, la salida de los estados ocultos se procesa aún más utilizando una capa fully connected. Dado que la salida de LSTM corresponde a un valor para cada paso de tiempo de entrada, se debe seleccionar solo la salida del último paso de tiempo.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:33:57.001329Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:32:38.711242Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;2599e2b7f31213a3e230d3fd87f5dca1&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-d106920d76b987cc&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="70">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># NOTEN QUE ESTOY PONIENDO DE NUEVO LOS SEEDS PARA SER CONSTANTES</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>random.seed(seed_)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed_)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed_)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed(seed_)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed_all(seed_)  <span class="co"># Multi-GPU.</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co">############</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CustomModelLSTM()</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizador y perdida</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters())</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Observen como podemos también definir un DataLoader de forma snecilla</span></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> data.DataLoader(data.TensorDataset(X_train, y_train), shuffle<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Pueden cambiar el número de epocas en esta ocasión con tal de llegar al valor de la metrica de desempeño</span></span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">6000</span></span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Perdidas</span></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>loss_train <span class="op">=</span> []</span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>loss_test <span class="op">=</span> []</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Iteramos sobre cada epoca</span></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colocamos el modelo en modo de entrenamiento</span></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cargamos los batches</span></span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> loader:</span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtenemos una primera prediccion</span></span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_batch)</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculamos la perdida</span></span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y_batch)</span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reseteamos la gradiente a cero</span></span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   sino la gradiente de previas iteraciones se acumulará con las nuevas</span></span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop</span></span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Aplicar las gradientes para actualizar los parametros del modelo</span></span>
<span id="cb60-46"><a href="#cb60-46" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb60-47"><a href="#cb60-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-48"><a href="#cb60-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validación cada 100 epocas</span></span>
<span id="cb60-49"><a href="#cb60-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">!=</span> <span class="dv">0</span> <span class="kw">and</span> epoch <span class="op">!=</span> n_epochs<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb60-50"><a href="#cb60-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb60-51"><a href="#cb60-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colocamos el modelo en modo de evaluación</span></span>
<span id="cb60-52"><a href="#cb60-52" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb60-53"><a href="#cb60-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-54"><a href="#cb60-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Deshabilitamos el calculo de gradientes</span></span>
<span id="cb60-55"><a href="#cb60-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb60-56"><a href="#cb60-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediccion</span></span>
<span id="cb60-57"><a href="#cb60-57" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb60-58"><a href="#cb60-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del RMSE - Root Mean Square Error</span></span>
<span id="cb60-59"><a href="#cb60-59" aria-hidden="true" tabindex="-1"></a>        train_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_train))</span>
<span id="cb60-60"><a href="#cb60-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Prediccion sobre validation</span></span>
<span id="cb60-61"><a href="#cb60-61" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_test)</span>
<span id="cb60-62"><a href="#cb60-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculo del RMSE para validation</span></span>
<span id="cb60-63"><a href="#cb60-63" aria-hidden="true" tabindex="-1"></a>        test_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_test))</span>
<span id="cb60-64"><a href="#cb60-64" aria-hidden="true" tabindex="-1"></a>        loss_train.append(train_rmse)</span>
<span id="cb60-65"><a href="#cb60-65" aria-hidden="true" tabindex="-1"></a>        loss_test.append(test_rmse)</span>
<span id="cb60-66"><a href="#cb60-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-67"><a href="#cb60-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Epoch </span><span class="sc">%d</span><span class="st">: train RMSE </span><span class="sc">%.4f</span><span class="st">, test RMSE </span><span class="sc">%.4f</span><span class="st">"</span> <span class="op">%</span> (epoch, train_rmse, test_rmse))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0: train RMSE 254.8949, test RMSE 449.7705
Epoch 100: train RMSE 207.2679, test RMSE 399.2317
Epoch 200: train RMSE 169.3058, test RMSE 357.2211
Epoch 300: train RMSE 138.3868, test RMSE 320.3750
Epoch 400: train RMSE 114.9555, test RMSE 288.6098
Epoch 500: train RMSE 90.6233, test RMSE 253.9066
Epoch 600: train RMSE 73.4680, test RMSE 224.7921
Epoch 700: train RMSE 60.4546, test RMSE 199.9395
Epoch 800: train RMSE 50.4180, test RMSE 178.4672
Epoch 900: train RMSE 44.4325, test RMSE 164.1384
Epoch 1000: train RMSE 40.2451, test RMSE 152.8020
Epoch 1100: train RMSE 35.5428, test RMSE 137.9217
Epoch 1200: train RMSE 32.3909, test RMSE 125.5889
Epoch 1300: train RMSE 29.8973, test RMSE 115.3716
Epoch 1400: train RMSE 28.1132, test RMSE 106.6467
Epoch 1500: train RMSE 27.1814, test RMSE 99.4058
Epoch 1600: train RMSE 26.0838, test RMSE 93.4244
Epoch 1700: train RMSE 25.4249, test RMSE 88.2113
Epoch 1800: train RMSE 24.8670, test RMSE 84.3252
Epoch 1900: train RMSE 24.1399, test RMSE 81.2719
Epoch 2000: train RMSE 23.8374, test RMSE 79.1533
Epoch 2100: train RMSE 23.3933, test RMSE 77.0301
Epoch 2200: train RMSE 22.9338, test RMSE 75.5494
Epoch 2300: train RMSE 23.5863, test RMSE 74.3277
Epoch 2400: train RMSE 23.5690, test RMSE 73.3854
Epoch 2500: train RMSE 22.1163, test RMSE 71.4037
Epoch 2600: train RMSE 22.0763, test RMSE 73.6913
Epoch 2700: train RMSE 22.7800, test RMSE 73.6697
Epoch 2800: train RMSE 22.0297, test RMSE 71.2587
Epoch 2900: train RMSE 22.8190, test RMSE 71.7467
Epoch 3000: train RMSE 22.1901, test RMSE 70.3245
Epoch 3100: train RMSE 22.0067, test RMSE 68.4189
Epoch 3200: train RMSE 21.1732, test RMSE 69.2261
Epoch 3300: train RMSE 22.6875, test RMSE 71.8242
Epoch 3400: train RMSE 21.3331, test RMSE 71.6772
Epoch 3500: train RMSE 22.0575, test RMSE 71.0764
Epoch 3600: train RMSE 21.6710, test RMSE 68.7575
Epoch 3700: train RMSE 21.3720, test RMSE 68.5613
Epoch 3800: train RMSE 21.5315, test RMSE 67.5496
Epoch 3900: train RMSE 21.4181, test RMSE 67.1450
Epoch 4000: train RMSE 21.0305, test RMSE 66.5857
Epoch 4100: train RMSE 21.1796, test RMSE 66.6339
Epoch 4200: train RMSE 21.3311, test RMSE 66.1776
Epoch 4300: train RMSE 21.1097, test RMSE 66.4625
Epoch 4400: train RMSE 21.6382, test RMSE 65.3451
Epoch 4500: train RMSE 21.2742, test RMSE 65.6760
Epoch 4600: train RMSE 21.2378, test RMSE 66.5480
Epoch 4700: train RMSE 21.0015, test RMSE 65.9074
Epoch 4800: train RMSE 20.5147, test RMSE 65.3427
Epoch 4900: train RMSE 20.4801, test RMSE 65.2674
Epoch 5000: train RMSE 20.4268, test RMSE 65.2035
Epoch 5100: train RMSE 20.7027, test RMSE 64.9880
Epoch 5200: train RMSE 20.5927, test RMSE 65.5623
Epoch 5300: train RMSE 20.2175, test RMSE 65.5491
Epoch 5400: train RMSE 19.9497, test RMSE 64.7060
Epoch 5500: train RMSE 20.5579, test RMSE 65.1167
Epoch 5600: train RMSE 19.9120, test RMSE 65.1008
Epoch 5700: train RMSE 20.1392, test RMSE 65.1234
Epoch 5800: train RMSE 20.0720, test RMSE 65.2581
Epoch 5900: train RMSE 21.4380, test RMSE 64.0588
Epoch 5999: train RMSE 19.8653, test RMSE 66.7324</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:33:57.188220Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:33:57.003832Z&quot;}" data-execution_count="71">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización del rendimiento</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> np.arange(<span class="bu">len</span>(loss_train))</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, loss_train, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Training'</span>,)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch, loss_test, <span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Test'</span>)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>), plt.ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-43-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:33:57.407236Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:33:57.190215Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;52fe33653ffb1624968f4a4a8b8dd877&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;cell-5a5264aa04158cad&quot;,&quot;locked&quot;:true,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="72">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficamos</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Movemos las predicciones de train para graficar</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    train_plot <span class="op">=</span> np.ones_like(time_series) <span class="op">*</span> np.nan</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prediccion de train</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extraemos los datos solo del ultimo paso</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> y_pred[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    train_plot[lb : num_train] <span class="op">=</span> model(X_train)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Movemos las predicciones de test</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    test_plot <span class="op">=</span> np.ones_like(time_series) <span class="op">*</span> np.nan</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    test_plot[num_train <span class="op">+</span> lb : <span class="bu">len</span>(time_series)] <span class="op">=</span> model(X_test)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>plt.plot(time_series, label<span class="op">=</span><span class="st">"Serie Original"</span>)</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>plt.plot(train_plot, c<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">"Serie Train"</span>)</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>plt.plot(test_plot, c<span class="op">=</span><span class="st">'g'</span>, label<span class="op">=</span><span class="st">"Serie Test"</span>)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Paso en el Tiempo'</span>), plt.ylabel(<span class="st">'Pasajeros'</span>)</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="lab3_files/figure-html/cell-44-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Nota:</strong> Lo que se estará evaluando es el RMSE tanto en training como en test. Se evaluará que en training sea <strong>menor a 22</strong>, mientras que en testing sea <strong>menor a 70</strong>.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:33:57.423482Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:33:57.410233Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;04af852d7a882ae7a5dddcd4fe42d22b&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-65c8e80376d46bc1&quot;,&quot;locked&quot;:true,&quot;points&quot;:28,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="73">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="bu">float</span>(loss_test[<span class="bu">len</span>(loss_test)<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="bu">float</span>(test_rmse)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>loss_train</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss_train[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> <span class="dv">22</span> </span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> train_rmse <span class="op">&lt;</span> <span class="dv">22</span> </span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> loss_test[<span class="op">-</span><span class="dv">1</span>] <span class="op">&lt;</span> <span class="dv">70</span> </span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tick.marks(<span class="dv">7</span>):        </span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> test_rmse <span class="op">&lt;</span> <span class="dv">70</span> </span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
<div class="cell-output cell-output-display">

        <div class="alert alert-box alert-success">
        <h1> <!--{id:"CORRECTMARK", marks:"7"}--> 
         ✓ [7 marks] 
         </h1> </div>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2023-07-31T02:33:57.438818Z&quot;,&quot;start_time&quot;:&quot;2023-07-31T02:33:57.425481Z&quot;}" data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5fc71d80805acbbec919a3972572b7f4&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;cell-a895611caee19d78&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}" data-execution_count="74">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio"</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>tick.summarise_marks() <span class="co"># </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio</code></pre>
</div>
<div class="cell-output cell-output-display">
<!--{id:"TOTALMARK",marks:"158", available:"158"}  -->
        
        <h1> 158 / 158 marks (100.0%) </h1>
        
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>